{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# introduction to the vectorizing principle\n",
    "\n",
    "## part 7: aggregation functions\n",
    "\n",
    "The core of the vectorizing principle is treating rows separately. In this part, we look on \n",
    "aggregation functions which group the data and perform vectorized operations on each group.\n",
    "\n",
    "#### the following code shows an aggregation function call in different libraries:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import ibis\n",
    "from ibis import _ as col\n",
    "import pydiverse.transform as pdt\n",
    "from pydiverse.pipedag import Flow, Stage, Table, materialize\n",
    "from pydiverse.transform import λ\n",
    "from pydiverse.transform.core.verbs import (\n",
    "    mutate, select, alias, group_by, summarise, arrange,\n",
    ")\n",
    "from pydiverse.transform.eager import PandasTableImpl\n",
    "from pydiverse.transform.lazy import SQLTableImpl\n",
    "\n",
    "\n",
    "@materialize(input_type=pd.DataFrame, version=\"1.0.0\")\n",
    "def task_pandas(titanic: pd.DataFrame):\n",
    "    return (\n",
    "        titanic\n",
    "        .assign(age_bucket=(titanic.age + 4.999).round(-1))\n",
    "        .groupby(\"age_bucket\")\n",
    "        .agg(samples=(\"age_bucket\", \"count\"), survival_likelyhood=(\"survived\", \"mean\"))\n",
    "        .sort_values(\"age_bucket\")\n",
    "    )\n",
    "\n",
    "\n",
    "@materialize(input_type=pl.DataFrame, version=\"1.0.0\")\n",
    "def task_polars(titanic: pl.DataFrame):\n",
    "    return (\n",
    "        titanic\n",
    "        .with_columns(age_bucket=(((pl.col(\"age\") + 4.999) / 10).round() * 10))\n",
    "        .groupby(\"age_bucket\")\n",
    "        .agg(samples=pl.col(\"age_bucket\").count(),\n",
    "             survival_likelyhood=pl.col(\"survived\").mean())\n",
    "        .sort(\"age_bucket\")\n",
    "    )\n",
    "\n",
    "\n",
    "@materialize(input_type=PandasTableImpl, version=\"1.0.0\")\n",
    "def task_transform_df(titanic: pdt.Table):\n",
    "    return (\n",
    "        titanic\n",
    "        >> mutate(age_bucket = round(λ.age + 4.999, -1))\n",
    "        >> group_by(λ.age_bucket)\n",
    "        >> summarise(samples=λ.age_bucket.count(),\n",
    "                    survival_likelyhood=λ.survived.mean())\n",
    "        >> arrange(λ.age_bucket)\n",
    "        >> alias(\"transform_df\")\n",
    "    )\n",
    "\n",
    "\n",
    "@materialize(input_type=SQLTableImpl, lazy=True)\n",
    "def task_transform_sql(titanic: pdt.Table):\n",
    "    return (\n",
    "        titanic\n",
    "        >> mutate(age_bucket = round(λ.age + 4.999, -1))\n",
    "        >> group_by(λ.age_bucket)\n",
    "        >> summarise(samples=λ.age_bucket.count(),\n",
    "                    survival_likelyhood=λ.survived.mean())\n",
    "        >> arrange(λ.age_bucket)\n",
    "        >> alias(\"transform_sql\")\n",
    "    )\n",
    "\n",
    "\n",
    "@materialize(input_type=ibis.api.Table, lazy=True)\n",
    "def task_ibis(titanic: ibis.api.Table):\n",
    "    return (\n",
    "        titanic\n",
    "        .mutate(age_bucket = (col.age + ibis.literal(4.999, \"decimal\")).round(-1))\n",
    "        .group_by(col.age_bucket)\n",
    "        .aggregate(samples=col.age_bucket.count(), survival_likelyhood=col.survived.mean())\n",
    "        .order_by(col.age_bucket)\n",
    "    )\n",
    "\n",
    "\n",
    "@materialize(input_type=sa.Table, lazy=True)\n",
    "def task_sqlalchemy(titanic: sa.Table):\n",
    "    age_bucket = sa.func.round(titanic.c.age + 4.999, -1).label(\"age_bucket\")\n",
    "    return sa.select(\n",
    "        age_bucket,\n",
    "        sa.func.count(age_bucket).label(\"samples\"),\n",
    "        sa.func.avg(titanic.c.survived).label(\"survival_likelyhood\")\n",
    "    ).select_from(titanic).group_by(age_bucket).order_by(age_bucket)\n",
    "\n",
    "\n",
    "@materialize(input_type=sa.Table, lazy=True)\n",
    "def task_sql(titanic: sa.Table):\n",
    "    return sa.text(f\"\"\"\n",
    "        SELECT round((titanic.age + 4.999) / CAST(10 AS NUMERIC)) * 10        AS age_bucket,\n",
    "               count(round((titanic.age + 4.999) / CAST(10 AS NUMERIC)) * 10) AS samples,\n",
    "               AVG(titanic.survived)                                          AS survival_likelyhood\n",
    "        FROM {titanic.original.schema}.{titanic.name} AS titanic\n",
    "        GROUP BY round((titanic.age + 4.999) / CAST(10 AS NUMERIC)) * 10\n",
    "        ORDER BY round((titanic.age + 4.999) / CAST(10 AS NUMERIC)) * 10 ASC NULLS LAST\n",
    "    \"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T17:00:52.280290760Z",
     "start_time": "2023-08-16T17:00:51.622050052Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define remaining tasks and helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "@materialize(version=\"1.0.0\")\n",
    "def read_input_data():\n",
    "    titanic = pd.read_csv(\n",
    "        'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "    )\n",
    "    return Table(titanic, name=\"titanic\")\n",
    "\n",
    "\n",
    "@materialize(input_type=pd.DataFrame, version=\"1.0.0\")\n",
    "def print_tables(tbls: list[pd.DataFrame]):\n",
    "    for tbl in tbls:\n",
    "        print(f\"\\n\\n{tbl}\")\n",
    "\n",
    "\n",
    "def get_pipeline():\n",
    "    tasks = [task_pandas, task_polars, task_transform_df, task_transform_sql,\n",
    "             task_ibis, task_sqlalchemy, task_sql]\n",
    "    with Flow(\"flow\") as flow:\n",
    "        with Stage(\"t1_raw_input\"):\n",
    "            titanic = read_input_data()\n",
    "\n",
    "        with Stage(\"t2_transformed_data\"):\n",
    "            out_tbls = [task(titanic) for task in tasks]\n",
    "            print_tables(out_tbls)\n",
    "\n",
    "    return flow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T17:00:52.284740825Z",
     "start_time": "2023-08-16T17:00:52.282636348Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define pipeline:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_pipeline():\n",
    "    tasks = [task_pandas, task_polars, task_transform_df, task_transform_sql,\n",
    "             task_ibis, task_sqlalchemy, task_sql]\n",
    "    with Flow(\"flow\") as flow:\n",
    "        with Stage(\"t1_raw_input\"):\n",
    "            titanic = read_input_data()\n",
    "\n",
    "        with Stage(\"t2_transformed_data\"):\n",
    "            out_tbls = [task(titanic) for task in tasks]\n",
    "            print_tables(out_tbls)\n",
    "\n",
    "    return flow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T17:00:52.328338448Z",
     "start_time": "2023-08-16T17:00:52.285573682Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### setup logging:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import logging\n",
    "from pydiverse.pipedag.util.structlog import setup_logging\n",
    "setup_logging(log_level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T17:00:52.328568407Z",
     "start_time": "2023-08-16T17:00:52.328220790Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### run pipeline:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m2023-08-16 17:00:52.401724\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mInitialized SQL Table Store   \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mengine_url\u001B[0m=\u001B[35mduckdb:////tmp/pipedag/vectorization/db.duckdb\u001B[0m \u001B[36mschema_prefix\u001B[0m=\u001B[35m\u001B[0m \u001B[36mschema_suffix\u001B[0m=\u001B[35m\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.408954\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStarting IPCServer            \u001B[0m [\u001B[34m\u001B[1mRunContextServer\u001B[0m] \u001B[36maddress\u001B[0m=\u001B[35mtcp://127.0.0.1:33309\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.412029\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mInitialized SQL Table Store   \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mengine_url\u001B[0m=\u001B[35mduckdb:////tmp/pipedag/vectorization/db.duckdb\u001B[0m \u001B[36mschema_prefix\u001B[0m=\u001B[35m\u001B[0m \u001B[36mschema_suffix\u001B[0m=\u001B[35m\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.449647\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS pipedag_metadata\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.475549\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS t1_raw_input\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.476739\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mDROP SCHEMA IF EXISTS t1_raw_input__even CASCADE\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.478269\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA t1_raw_input__even\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.503938\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t1_raw_input__even.titanic AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t1_raw_input.titanic\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.513918\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'read_input_data'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'read_input_data' 0x7f3a5956cfd0 (id: 0)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.515893\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'read_input_data'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'read_input_data' 0x7f3a5956cfd0 (id: 0)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.517111\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mCommitting stage              \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: t1_raw_input>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.519394\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStage is cache valid          \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: t1_raw_input>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.526350\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: t1_raw_input>\u001B[0m \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.COMPLETED: 1>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.529853\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS t2_transformed_data\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.530633\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mDROP SCHEMA IF EXISTS t2_transformed_data__odd CASCADE\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.531983\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA t2_transformed_data__odd\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.545891\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.task_pandas_knng4lw75uvluken764b_0000 AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.task_pandas_knng4lw75uvluken764b_0000\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.552402\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'task_pandas'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_pandas' 0x7f39e8220430 (id: 2)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.554981\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_pandas'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_pandas' 0x7f39e8220430 (id: 2)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.564964\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.task_polars_yr2b4t44onqdejzgmsom_0000 AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.task_polars_yr2b4t44onqdejzgmsom_0000\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.571621\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'task_polars'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_polars' 0x7f39e91365c0 (id: 3)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.573669\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_polars'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_polars' 0x7f39e91365c0 (id: 3)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.584870\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.transform_df AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.transform_df\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.592361\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'task_transform_df'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_transform_df' 0x7f39e8ceadd0 (id: 4)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.594128\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_transform_df'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_transform_df' 0x7f39e8ceadd0 (id: 4)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.599506\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFailed to retrieve table from local table cache\u001B[0m [\u001B[34m\u001B[1mParquetTableCache\u001B[0m] \u001B[36mcause\u001B[0m=\u001B[35mCan't retrieve Table as type <class 'pydiverse.transform.lazy.sql_table.SQLTableImpl'>\u001B[0m \u001B[36mtable\u001B[0m=\u001B[35m<Table 'titanic' (t1_raw_input)>\u001B[0m\n",
      "/home/martin/progs/miniconda3/envs/vectorization/lib/python3.10/site-packages/duckdb_engine/__init__.py:162: DuckDBEngineWarning: duckdb-engine doesn't yet support reflection on indices\n",
      "  warnings.warn(\n",
      "\u001B[2m2023-08-16 17:00:52.668640\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.transform_sql AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.transform_sql\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.673714\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mLazy cache of table 'transform_sql' found\u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "\u001B[2m2023-08-16 17:00:52.682738\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_transform_sql'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_transform_sql' 0x7f39e80f46d0 (id: 5)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.689514\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFailed to retrieve table from local table cache\u001B[0m [\u001B[34m\u001B[1mParquetTableCache\u001B[0m] \u001B[36mcause\u001B[0m=\u001B[35mCan't retrieve Table as type <class 'ibis.expr.types.relations.Table'>\u001B[0m \u001B[36mtable\u001B[0m=\u001B[35m<Table 'titanic' (t1_raw_input)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.922517\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.task_ibis_cglsqmokt47hkhkien6h_0000 AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.task_ibis_cglsqmokt47hkhkien6h_0000\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.926509\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mLazy cache of table 'task_ibis_cglsqmokt47hkhkien6h_0000' found\u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "\u001B[2m2023-08-16 17:00:52.933846\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_ibis'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_ibis' 0x7f39e80f4700 (id: 6)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:52.940073\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFailed to retrieve table from local table cache\u001B[0m [\u001B[34m\u001B[1mParquetTableCache\u001B[0m] \u001B[36mcause\u001B[0m=\u001B[35mCan't retrieve Table as type <class 'sqlalchemy.sql.schema.Table'>\u001B[0m \u001B[36mtable\u001B[0m=\u001B[35m<Table 'titanic' (t1_raw_input)>\u001B[0m\n",
      "/home/martin/progs/miniconda3/envs/vectorization/lib/python3.10/site-packages/duckdb_engine/__init__.py:162: DuckDBEngineWarning: duckdb-engine doesn't yet support reflection on indices\n",
      "  warnings.warn(\n",
      "\u001B[2m2023-08-16 17:00:52.999216\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.task_sqlalchemy_uuepwjyooeeuxnbzzgme_0000 AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.task_sqlalchemy_uuepwjyooeeuxnbzzgme_0000\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.003539\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mLazy cache of table 'task_sqlalchemy_uuepwjyooeeuxnbzzgme_0000' found\u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "\u001B[2m2023-08-16 17:00:53.011995\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_sqlalchemy'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_sqlalchemy' 0x7f39e8259f60 (id: 7)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.017492\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFailed to retrieve table from local table cache\u001B[0m [\u001B[34m\u001B[1mParquetTableCache\u001B[0m] \u001B[36mcause\u001B[0m=\u001B[35mCan't retrieve Table as type <class 'sqlalchemy.sql.schema.Table'>\u001B[0m \u001B[36mtable\u001B[0m=\u001B[35m<Table 'titanic' (t1_raw_input)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.078740\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW t2_transformed_data__odd.task_sql_zvt3ug3l35bjetdswfjq_0000 AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM t2_transformed_data.task_sql_zvt3ug3l35bjetdswfjq_0000\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.082817\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mLazy cache of table 'task_sql_zvt3ug3l35bjetdswfjq_0000' found\u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "\u001B[2m2023-08-16 17:00:53.091852\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'task_sql'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'task_sql' 0x7f39e825a8f0 (id: 8)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.099727\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'print_tables'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'print_tables' 0x7f39e8220760 (id: 9)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.101577\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'print_tables'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'print_tables' 0x7f39e8220760 (id: 9)>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.102869\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mCommitting stage              \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: t2_transformed_data>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.106057\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStage is cache valid          \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: t2_transformed_data>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.112791\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: t2_transformed_data>\u001B[0m \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.COMPLETED: 1>\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.314581\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStopped IPCServer             \u001B[0m [\u001B[34m\u001B[1mRunContextServer\u001B[0m] \u001B[36maddress\u001B[0m=\u001B[35mtcp://127.0.0.1:33309\u001B[0m\n",
      "\u001B[2m2023-08-16 17:00:53.326505\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFlow visualization            \u001B[0m [\u001B[34m\u001B[1mFlow\u001B[0m] \u001B[36murl\u001B[0m=\u001B[35mhttps://kroki.io/graphviz/svg/eNqlkl1rwyAUhu_9FZLddpCl-SqS3e5HjCEn0bTS0yRTwyij_31JdWMf0rTMmwePPK-eg0JtNQw7-kTfiRlrt2lwNFZqbh-4hjeuumG003m9bXrsdRXdxeeVxBEjrlQjNHtG2r6zPwoItcTqew4jxh5RVqZHJRiJ6XOrED-DZdm2TR6tqBO1BOE0LsDCijo3mhUpohdGTiTw7IRbDZ1pe32Q4mz-6_V_4341sb7YhAWz50OPoE2ggXzZVbUKmcWyaV4RsNnJwzHgby76g1ad5RZqlKHLkytahk5AyE2X3a-Bc9EGErJbEqYhBCLKq6YX_nExvX-kCXNce6aemWfuWXiW89Ambub_4ph6Zp65Z-FZep7IB7IGLYA=\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.SVG object>",
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1034pt\" height=\"262pt\" viewBox=\"0.00 0.00 1034.00 262.31\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 258.31)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-258.31 1030,-258.31 1030,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_t1_raw_input</title>\n<polygon fill=\"#000000\" fill-opacity=\"0.125490\" stroke=\"black\" points=\"467,-167.54 467,-246.31 629,-246.31 629,-167.54 467,-167.54\"/>\n<text text-anchor=\"middle\" x=\"548\" y=\"-229.01\" font-family=\"Times,serif\" font-size=\"14.00\">t1_raw_input</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_t2_transformed_data</title>\n<polygon fill=\"#000000\" fill-opacity=\"0.125490\" stroke=\"black\" points=\"8,-8 8,-159.54 1018,-159.54 1018,-8 8,-8\"/>\n<text text-anchor=\"middle\" x=\"513\" y=\"-142.24\" font-family=\"Times,serif\" font-size=\"14.00\">t2_transformed_data</text>\n</g>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"548\" cy=\"-193.92\" rx=\"72.83\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"548\" y=\"-189.62\" font-family=\"Times,serif\" font-size=\"14.00\">read_input_data</text>\n</g>\n<!-- 3 -->\n<g id=\"node2\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"955\" cy=\"-107.15\" rx=\"54.8\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"955\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_polars</text>\n</g>\n<!-- 0&#45;&gt;3 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0-&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M619.91,-190.74C705.7,-187.3 843.49,-178.89 891,-159.54 905.93,-153.46 920.17,-142.71 931.41,-132.62\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"933.5,-135.51 938.38,-126.11 928.7,-130.42 933.5,-135.51\"/>\n</g>\n<!-- 6 -->\n<g id=\"node3\" class=\"node\">\n<title>6</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"837\" cy=\"-107.15\" rx=\"45.25\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"837\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_ibis</text>\n</g>\n<!-- 0&#45;&gt;6 -->\n<g id=\"edge5\" class=\"edge\">\n<title>0-&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M617.16,-187.78C675.71,-182.58 754.53,-173.32 783,-159.54 795.92,-153.28 807.79,-142.8 817.08,-132.93\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"819.21,-135.68 823.25,-125.88 813.98,-131.02 819.21,-135.68\"/>\n</g>\n<!-- 7 -->\n<g id=\"node4\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"699\" cy=\"-107.15\" rx=\"74.95\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"699\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_sqlalchemy</text>\n</g>\n<!-- 0&#45;&gt;7 -->\n<g id=\"edge6\" class=\"edge\">\n<title>0-&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M581.01,-177.18C591.93,-171.79 604.09,-165.58 615,-159.54 631.47,-150.42 649.32,-139.63 664.31,-130.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"665.62,-132.99 672.24,-124.72 661.9,-127.06 665.62,-132.99\"/>\n</g>\n<!-- 2 -->\n<g id=\"node6\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"548\" cy=\"-107.15\" rx=\"57.98\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"548\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_pandas</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M548,-175.36C548,-164.18 548,-149.45 548,-136.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"551.5,-136.99 548,-126.99 544.5,-136.99 551.5,-136.99\"/>\n</g>\n<!-- 4 -->\n<g id=\"node7\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"390\" cy=\"-107.15\" rx=\"81.85\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"390\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_transform_df</text>\n</g>\n<!-- 0&#45;&gt;4 -->\n<g id=\"edge3\" class=\"edge\">\n<title>0-&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M515.45,-177.13C504.45,-171.68 492.13,-165.45 481,-159.54 463.32,-150.15 443.97,-139.31 427.68,-130.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"429.9,-126.67 419.48,-124.74 426.42,-132.75 429.9,-126.67\"/>\n</g>\n<!-- 5 -->\n<g id=\"node8\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"205\" cy=\"-107.15\" rx=\"85.03\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"205\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_transform_sql</text>\n</g>\n<!-- 0&#45;&gt;5 -->\n<g id=\"edge4\" class=\"edge\">\n<title>0-&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M476.15,-190.25C425.56,-186.7 356.82,-178.49 299,-159.54 277.97,-152.65 256.19,-141.06 238.89,-130.64\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"241.12,-127.28 230.78,-125 237.44,-133.24 241.12,-127.28\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"59\" cy=\"-107.15\" rx=\"42.6\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"59\" y=\"-102.85\" font-family=\"Times,serif\" font-size=\"14.00\">task_sql</text>\n</g>\n<!-- 0&#45;&gt;8 -->\n<g id=\"edge7\" class=\"edge\">\n<title>0-&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M474.92,-193.37C367.76,-193.01 173.91,-188.23 111,-159.54 98.19,-153.7 86.72,-143.27 77.83,-133.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"80.11,-131.6 71,-126.2 74.75,-136.11 80.11,-131.6\"/>\n</g>\n<!-- 9 -->\n<g id=\"node5\" class=\"node\">\n<title>9</title>\n<ellipse fill=\"#e8ffc6\" stroke=\"black\" cx=\"548\" cy=\"-34.38\" rx=\"56.92\" ry=\"18.38\"/>\n<text text-anchor=\"middle\" x=\"548\" y=\"-30.08\" font-family=\"Times,serif\" font-size=\"14.00\">print_tables</text>\n</g>\n<!-- 3&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>3-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M914.54,-94.44C906.76,-92.39 898.66,-90.4 891,-88.77 794.21,-68.16 680.46,-52.02 611.33,-43.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"611.99,-39.53 601.63,-41.73 611.1,-46.47 611.99,-39.53\"/>\n</g>\n<!-- 6&#45;&gt;9 -->\n<g id=\"edge12\" class=\"edge\">\n<title>6-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M802.86,-94.66C796.3,-92.6 789.46,-90.54 783,-88.77 723.29,-72.41 654.09,-57.13 606.32,-47.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"607.07,-43.51 596.57,-44.91 605.65,-50.37 607.07,-43.51\"/>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge13\" class=\"edge\">\n<title>7-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M665.49,-90.45C642.98,-79.9 613.18,-65.93 589.23,-54.71\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"590.9,-51.16 580.36,-50.08 587.93,-57.49 590.9,-51.16\"/>\n</g>\n<!-- 2&#45;&gt;9 -->\n<g id=\"edge8\" class=\"edge\">\n<title>2-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M548,-88.3C548,-80.86 548,-72.05 548,-63.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"551.5,-63.99 548,-53.99 544.5,-63.99 551.5,-63.99\"/>\n</g>\n<!-- 4&#45;&gt;9 -->\n<g id=\"edge10\" class=\"edge\">\n<title>4-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M425.46,-90.27C449.21,-79.63 480.59,-65.58 505.66,-54.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"506.92,-57.17 514.62,-49.89 504.06,-50.78 506.92,-57.17\"/>\n</g>\n<!-- 5&#45;&gt;9 -->\n<g id=\"edge11\" class=\"edge\">\n<title>5-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M265.14,-93.75C328.62,-80.65 427.95,-60.15 490.44,-47.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"490.81,-50.55 499.9,-45.1 489.4,-43.7 490.81,-50.55\"/>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge14\" class=\"edge\">\n<title>8-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M90.61,-94.46C97.28,-92.31 104.31,-90.28 111,-88.77 240.28,-59.58 394.92,-45.44 481.25,-39.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"481.46,-42.78 491.2,-38.6 480.98,-35.79 481.46,-42.78\"/>\n</g>\n</g>\n</svg>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flow = get_pipeline()\n",
    "result = flow.run()\n",
    "result.visualize()\n",
    "assert result.successful"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T17:00:53.444536904Z",
     "start_time": "2023-08-16T17:00:52.328441425Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
