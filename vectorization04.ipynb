{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# introduction to the vectorizing principle\n",
    "\n",
    "## part 4: defining a data pipeline\n",
    "\n",
    "Here, we are focusing on data transformation code used for curating data or training and evaluating a model. In this case, a data pipeline can be used as a means of splitting the work in pieces that are conceptionally relevant for creating, operating, or maintaining the code. One can see a data pipeline as a directed acyclic graph (DAG) of tasks. Often a hierarchical notion of grouping the tasks is useful for example grouping them in stages. Typical stages of a data pipeline may be:\n",
    "\n",
    "1. raw data ingestion\n",
    "2. early cleaning for easier inspection\n",
    "3. transformation in the best possible form for subject matter reasoning\n",
    "4. feature enrichment\n",
    "5. model training\n",
    "6. model evaluation\n",
    "\n",
    "Data pipeline orchestration tools can help with tracing/monitoring different executions of the pipeline. They may help with communication between tasks as well as materializations/dematerializations to persistent storage (files or databases). They may introduce transactionality features like showing the result of a stage only on success. And they can help with caching and automatic cache-invalidation. It helps to think of intermediate data within a pipeline as caches since they (should) only depend on external inputs into the pipeline and the code running it. Often a notion of caching is implemented in an ad-hoc fasion with manual control but this causes trouble if multiple instances of the pipeline should be operated in order to ensure an agile development process working with small, medium, large input data both with per-user and team shared caches. For most data analytic pipelines, the algorithm is not fixed upfront. It is developed as part of the creation process. Thus, a high development iteration speed is key! Good software design and reproduction/testing strategies are needed to keep up the speed long enough to create maximum value.\n",
    "\n",
    "#### define tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydiverse.pipedag.materialize import Blob, Table, materialize\n",
    "import lightgbm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "@materialize(version=\"1.0.0\")\n",
    "def read_input_data():\n",
    "    data_df = pd.read_csv('data/taxi_data/train.csv.gz')\n",
    "    return Table(data_df, name=\"input_data\")\n",
    "\n",
    "\n",
    "@materialize(version=\"1.0.0\", input_type=pd.DataFrame)\n",
    "def feature_trip_distance(df: pd.DataFrame):\n",
    "    start_lat = np.radians(df[\"pickup_latitude\"])\n",
    "    start_lng = np.radians(df[\"pickup_longitude\"])\n",
    "    dest_lat = np.radians(df[\"dropoff_latitude\"])\n",
    "    dest_lng = np.radians(df[\"dropoff_longitude\"])\n",
    "\n",
    "    d = (\n",
    "        np.sin(dest_lat / 2 - start_lat / 2) ** 2\n",
    "        + np.cos(start_lat)\n",
    "        * np.cos(dest_lat)\n",
    "        * np.sin(dest_lng / 2 - start_lng / 2) ** 2\n",
    "    )\n",
    "\n",
    "    return Table(pd.DataFrame(\n",
    "        dict(\n",
    "            id=df[\"id\"],\n",
    "            # 6,371 km is the earth radius\n",
    "            haversine_distance = 2 * 6371 * np.arcsin(np.sqrt(d))\n",
    "        )\n",
    "    ), name=\"trip_distance\")\n",
    "\n",
    "@materialize(version=\"1.0.0\", input_type=pd.DataFrame)\n",
    "def feature_split_pickup_datetime(df: pd.DataFrame):\n",
    "    tpep_pickup_datetime = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "    return Table(pd.DataFrame(\n",
    "        dict(\n",
    "            id=df[\"id\"],\n",
    "            pickup_dayofweek=tpep_pickup_datetime.dt.dayofweek,\n",
    "            pickup_hour=tpep_pickup_datetime.dt.hour,\n",
    "            pickup_minute=tpep_pickup_datetime.dt.minute,\n",
    "        )\n",
    "    ), name=\"pickup_datetime\")\n",
    "\n",
    "@materialize(nout=2, version=\"1.0.0\", input_type=pd.DataFrame)\n",
    "def get_feature_df(df: pd.DataFrame, features: list[pd.DataFrame], target_col=\"trip_duration\"):\n",
    "    final_df = df[[\"id\"] + [col for col in df.columns if col != target_col and df[col].dtype in (int, float, bool)]]\n",
    "    for feature_df in features:\n",
    "        final_df = final_df.merge(feature_df, on=\"id\")\n",
    "    return (\n",
    "        Table(final_df, name=\"features\"),\n",
    "        Table(df[[\"id\", target_col]], name=\"target\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def combine_features(data_df):\n",
    "    features = [\n",
    "        feature_trip_distance(data_df),\n",
    "        feature_split_pickup_datetime(data_df),\n",
    "    ]\n",
    "    return get_feature_df(data_df, features)\n",
    "\n",
    "\n",
    "@materialize(nout=4, version=\"1.0.0\", input_type=pd.DataFrame)\n",
    "def split_train_test(features_df: pd.DataFrame, target_df: pd.DataFrame):\n",
    "    features_df.sort_values(\"id\", inplace=True)\n",
    "    target_df.sort_values(\"id\", inplace=True)\n",
    "    (\n",
    "        features_train,\n",
    "        features_test,\n",
    "        target_train,\n",
    "        target_test,\n",
    "    ) = train_test_split(features_df, target_df, test_size=0.1)\n",
    "    return (\n",
    "        Table(features_train, name=\"features_train\"),\n",
    "        Table(features_test, name=\"features_test\"),\n",
    "        Table(target_train, name=\"target_train\"),\n",
    "        Table(target_test, name=\"target_test\"),\n",
    "    )\n",
    "\n",
    "@materialize(version=\"1.0.0\", input_type=pd.DataFrame)\n",
    "def train_model(features_train: pd.DataFrame, target_train: pd.DataFrame):\n",
    "    features_train.sort_values(\"id\", inplace=True)\n",
    "    target_train.sort_values(\"id\", inplace=True)\n",
    "    del target_train[\"id\"]\n",
    "    model = lightgbm.LGBMRegressor(objective=\"regression_l1\")\n",
    "    model.fit(features_train, target_train)\n",
    "    return Blob(model, name=\"model\")\n",
    "\n",
    "\n",
    "@materialize(version=\"1.0.0\", input_type=pd.DataFrame)\n",
    "def evaluate_model(features_test: pd.DataFrame, target_test: pd.DataFrame, model: lightgbm.LGBMRegressor):\n",
    "    features_test.sort_values(\"id\", inplace=True)\n",
    "    target_test.sort_values(\"id\", inplace=True)\n",
    "    del target_test[\"id\"]\n",
    "    predicted = model.predict(features_test)\n",
    "    print(model.score(features_test, target_test))\n",
    "    print(math.sqrt(mean_squared_error(target_test, predicted)))\n",
    "    lightgbm.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### define pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pydiverse.pipedag.core import Flow, PipedagConfig, Stage\n",
    "def get_pipeline():\n",
    "    with Flow(\"vectorization\") as flow:\n",
    "        with Stage(\"raw_input\"):\n",
    "            data_df = read_input_data()\n",
    "        with Stage(\"features\"):\n",
    "            features_df, target_df = combine_features(data_df)\n",
    "            (\n",
    "                features_train,\n",
    "                features_test,\n",
    "                target_train,\n",
    "                target_test,\n",
    "            ) = split_train_test(features_df, target_df)\n",
    "        with Stage(\"model\"):\n",
    "            model = train_model(features_train, target_train)\n",
    "        with Stage(\"evaluation\"):\n",
    "            evaluate_model(features_test, target_test, model)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### setup logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pydiverse.pipedag.util.structlog import setup_logging\n",
    "setup_logging(log_level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### run pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m2023-08-06 10:57:38.341381\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mInitialized SQL Table Store   \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mengine_url\u001B[0m=\u001B[35mduckdb:////tmp/pipedag/vectorization/db.duckdb\u001B[0m \u001B[36mschema_prefix\u001B[0m=\u001B[35m\u001B[0m \u001B[36mschema_suffix\u001B[0m=\u001B[35m\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.344856\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStarting IPCServer            \u001B[0m [\u001B[34m\u001B[1mRunContextServer\u001B[0m] \u001B[36maddress\u001B[0m=\u001B[35mtcp://127.0.0.1:42391\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.348427\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mInitialized SQL Table Store   \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mengine_url\u001B[0m=\u001B[35mduckdb:////tmp/pipedag/vectorization/db.duckdb\u001B[0m \u001B[36mschema_prefix\u001B[0m=\u001B[35m\u001B[0m \u001B[36mschema_suffix\u001B[0m=\u001B[35m\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.360762\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS pipedag_metadata\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.372303\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS raw_input\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.373897\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mDROP SCHEMA IF EXISTS raw_input__even CASCADE\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.376165\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA raw_input__even\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.418170\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW raw_input__even.input_data AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM raw_input.input_data\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.430502\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'read_input_data'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'read_input_data' 0x7fe850c87220 (id: 0)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.434469\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'read_input_data'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'read_input_data' 0x7fe850c87220 (id: 0)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.436847\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mCommitting stage              \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: raw_input>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.443446\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStage is cache valid          \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: raw_input>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.454427\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: raw_input>\u001B[0m \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.COMPLETED: 1>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.462922\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS features\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.464644\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mDROP SCHEMA IF EXISTS features__even CASCADE\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.467516\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA features__even\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.513086\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.trip_distance AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.trip_distance\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.533887\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'feature_trip_distance'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'feature_trip_distance' 0x7fe850c87580 (id: 2)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.541298\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'feature_trip_distance'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'feature_trip_distance' 0x7fe850c87580 (id: 2)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.567088\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.pickup_datetime AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.pickup_datetime\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.580026\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'feature_split_pickup_datetime'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'feature_split_pickup_datetime' 0x7fe850c861d0 (id: 3)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.584372\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'feature_split_pickup_datetime'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'feature_split_pickup_datetime' 0x7fe850c861d0 (id: 3)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.609719\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.features AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.features\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.624967\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.target AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.target\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.639878\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'get_feature_df'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'get_feature_df' 0x7fe850c87670 (id: 4)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.644779\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'get_feature_df'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'get_feature_df' 0x7fe850c87670 (id: 4)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.661988\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.features_train AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.features_train\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.676193\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.features_test AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.features_test\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.691794\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.target_train AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.target_train\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.704613\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE VIEW features__even.target_test AS\n",
      "    \u001B[35mSELECT * \n",
      "    \u001B[35mFROM features.target_test\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.714899\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'split_train_test'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'split_train_test' 0x7fe850c876d0 (id: 5)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.718776\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'split_train_test'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'split_train_test' 0x7fe850c876d0 (id: 5)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.721317\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mCommitting stage              \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: features>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.726827\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStage is cache valid          \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: features>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.735731\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: features>\u001B[0m \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.COMPLETED: 1>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.743348\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS model\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.744448\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mDROP SCHEMA IF EXISTS model__odd CASCADE\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.745989\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA model__odd\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.765126\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'train_model'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'train_model' 0x7fe850c86c20 (id: 7)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.769060\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'train_model'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'train_model' 0x7fe850c86c20 (id: 7)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.771756\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mCommitting stage              \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: model>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.776852\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStage is cache valid          \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: model>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.786490\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: model>\u001B[0m \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.COMPLETED: 1>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.794445\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA IF NOT EXISTS evaluation\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.795767\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mDROP SCHEMA IF EXISTS evaluation__odd CASCADE\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.797834\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mExecuting sql                 \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \n",
      "    [\u001B[36mquery\u001B[0m]\n",
      "    \u001B[35mCREATE SCHEMA evaluation__odd\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.814194\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFound task in cache. Using cached result.\u001B[0m [\u001B[34m\u001B[1mTask 'evaluate_model'\u001B[0m] \u001B[36mtask\u001B[0m=\u001B[35m<Task 'evaluate_model' 0x7fe850c87d30 (id: 9)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.818418\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mTask 'evaluate_model'\u001B[0m] \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.CACHE_VALID: 2>\u001B[0m \u001B[36mtask\u001B[0m=\u001B[35m<Task 'evaluate_model' 0x7fe850c87d30 (id: 9)>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.820530\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mCommitting stage              \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: evaluation>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.826102\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStage is cache valid          \u001B[0m [\u001B[34m\u001B[1mDuckDBTableStore\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: evaluation>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:38.835672\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mTask finished successfully    \u001B[0m [\u001B[34m\u001B[1mCommit Stage\u001B[0m] \u001B[36mstage\u001B[0m=\u001B[35m<Stage: evaluation>\u001B[0m \u001B[36mstate\u001B[0m=\u001B[35m<FinalTaskState.COMPLETED: 1>\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:39.038500\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mStopped IPCServer             \u001B[0m [\u001B[34m\u001B[1mRunContextServer\u001B[0m] \u001B[36maddress\u001B[0m=\u001B[35mtcp://127.0.0.1:42391\u001B[0m\n",
      "\u001B[2m2023-08-06 10:57:39.064979\u001B[0m [\u001B[32m\u001B[1minfo     \u001B[0m] \u001B[1mFlow visualization            \u001B[0m [\u001B[34m\u001B[1mFlow\u001B[0m] \u001B[36murl\u001B[0m=\u001B[35mhttps://kroki.io/graphviz/svg/eNqlklFrwyAUhd_9FZK9dhCSdF2R7HU_Ygwx8SaT2ih6szFG__tqTDaWhRCoLwevnMPnUalaJ-wbfaZfxPdV3NS69wiOO_HBVWd7vB5WbW20cWVylw4rSxNG4qjSoj4x0pgO_wy0qECXPyGMePzUUHqjlWQkpS-N0npKhcemqR-SHR1dIGS0cSlQ7Gj0JsECMnll5EL-AzcgsHfgb-GdMma42SruaOLolOVSeRRdDQvQxWpKCzhdgctmwZ5vgvBWK-RW1afehvYA1XkJZr-aFlPQCdVxBI8bn-BsJOhb-h8CZuUfVkkj4-DbCAnvQvcCleluIf1NmeEeV3FHG6wRp_T-iWYsaj5qEX5h1HzUIug-POVVD6MeQ2FRL-QbpJpJ5g==\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "flow = get_pipeline()\n",
    "result = flow.run()\n",
    "assert result.successful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
